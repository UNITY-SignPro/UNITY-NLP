# -*- coding: utf-8 -*-
"""NLP-DB.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1s3iMUYhgfh8gDD6MK4nw8wan0GwfYmyi
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get update
# apt-get install g++ openjdk-8-jdk python-dev python3-dev
# pip3 install JPype1
# pip3 install konlpy

from eunjeon import Mecab
import pandas as pd
import numpy as np
import re
# drive.mount('/content/gdrive')

import sqlite3
# conn = sqlite3.connect('./function/words.db')
conn = sqlite3.connect('word.db')
excel_sheet_1 = pd.read_csv('인간.csv')

c = conn.cursor()  # 커서 생성

c.execute('''CREATE TABLE IF NOT EXISTS word
            (origin text, rename text)''')

# c.execute('''CREATE TABLE word (origin text, chanword text)''')


# type(excel_sheet_1)
excel_sheet_1=list(np.array(excel_sheet_1.values.tolist()))


def formmating(word):
    return re.sub(pattern=r'\([^)]*\)', repl='', string= word).strip()

for origin, rename in iter(excel_sheet_1):
# for idx in range(len(excel_sheet_1)):
    # print(formmating(origin))
    c.execute("INSERT INTO word VALUES(?,?);", (formmating(origin), formmating(rename)))

price=c.execute("SELECT * FROM word")
price = c.fetchall()
print(price)



import konlpy
from konlpy.tag import Kkma, Komoran, Hannanum, Okt
from konlpy.utils import pprint
#from konlpy.tag import Mecab

mecab = Mecab()



def chan(word):
    a=c.execute(f"SELECT * FROM word WHERE origin = '{word}'").fetchone()
    print(">>>", a)
    # a=list(a)
    return a



import nltk
import json 

# POS tag a sentence
sentence= input("문장 입력하시오:")
words = Mecab().pos(sentence)
outplist= ""
verb=""
noun=""

verb_re=[]
noun_re=[]
# Define a chunk grammar, or chunking rules, then chunk
grammar = """
NP: {<N.*>*<Suffix>?}   # Noun phrase
VV: {<V.*>*}            # Verb phrase
AP: {<VA.*>*}            # Adjective phrase
MP: {<M.*>*}
EC: {<EC.*>*}
"""
parser = nltk.RegexpParser(grammar)
chunks = parser.parse(words)
print("# Print whole tree")

print(chunks.pprint())


for subtree in chunks.subtrees():
    if subtree.label()=='NP':
        print(' '.join((e[0] for e in list(subtree))))
        outplist= outplist+' '.join((e[0] for e in list(subtree)))+", "
        noun= noun+' '.join((e[0] for e in list(subtree)))+", "
    if  subtree.label()=='VV':
        print(' '.join((e[0] for e in list(subtree))))
        outplist= outplist+' '.join((e[
            0] for e in list(subtree)))+"다, "
        verb= verb+' '.join((e[0] for e in list(subtree)))+"다, "
        
    if  subtree.label()=='AP':
        print(' '.join((e[0] for e in list(subtree))))
        outplist= outplist+' '.join((e[0] for e in list(subtree)))+"다, "
        verb= verb+' '.join((e[0] for e in list(subtree)))+"다, "
    if  subtree.label()=='MP':
        print(' '.join((e[0] for e in list(subtree))))
        outplist= outplist+' '.join((e[0] for e in list(subtree)))+", "
        noun= noun+' '.join((e[0] for e in list(subtree)))+", "

verb=verb.split(",")
for i in verb:
  print(">!>", i)
  print(chan(i))

noun=noun.split(",")
for i in noun:
  noun_re=chan(i)
  print(i)
json_object = {
    "noun": noun,
    "verb": verb
}

with open('output.json', 'w') as f:
    json.dump(json_object, f, indent=2)
      
outplist=outplist.split(",")

print(outplist)
# for i in outplist:
#   print(i)
#   print(chan(outplist))